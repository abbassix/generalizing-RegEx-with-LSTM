{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regex_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnx9+IzDjFGHpBWI/RRMWr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abbassix/generalizing-RegEx-with-LSTM/blob/main/regex_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-EShu_YR7Xn"
      },
      "source": [
        "# generate simple regexes like ab[ca]db\n",
        "def regex_generator(length=4.79, n_det=0, n_undet=0, letters='abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ', nails_length=4):\n",
        "  import random\n",
        "  def inject(string, positions, nails):\n",
        "    positions = [0] + sorted(positions * 2) + [len(string)]\n",
        "    positions = [(positions[i], positions[i+1]) for i in range(0, len(positions), 2)]\n",
        "    partitions = [string[i:j] for (i, j) in positions]\n",
        "    return(''.join([p_+n_ for p_, n_ in zip(partitions, nails)])+partitions[-1])\n",
        "\n",
        "  if n_det == 0:\n",
        "    n_det = max(round(random.gauss(length, 1)), 3)\n",
        "  if n_undet == 0:\n",
        "    n_undet = random.randint(1, round((n_det-1)/2))\n",
        "    \n",
        "  positions = sorted([random.randint(1, int(length) - 1) for _ in range(n_undet)])\n",
        "    \n",
        "  regex_det = ''.join([random.choice(letters) for i in range(n_det)])\n",
        "    \n",
        "  nails = ['[' + ''.join([random.choice(letters) for _ in range(random.randint(2, nails_length))]) + ']' for _ in range(n_undet)]\n",
        "\n",
        "  regex_sample = inject(regex_det, positions, nails)\n",
        "\n",
        "  return(regex_sample)\n",
        "\n",
        "# TO DO: more complex structures"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsQLX2TNSBNy",
        "outputId": "1119ae65-d67e-48db-ff08-e8eaf1d5b960"
      },
      "source": [
        "# install rstr to make samples from regexes as training data\n",
        "!pip install rstr"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rstr\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/30/1e10f00ea45c899ab946f9bde77be49e796a28cb96797ab8b249566490ef/rstr-2.2.6-py2.py3-none-any.whl\n",
            "Installing collected packages: rstr\n",
            "Successfully installed rstr-2.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK5rvmAGSEig"
      },
      "source": [
        "# decode regexes into samples\n",
        "def regex_decoder(regex):\n",
        "  import rstr\n",
        "  return(rstr.xeger(regex))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P43T79dTSIOb"
      },
      "source": [
        "# generate 5000 regexes\n",
        "letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "y = [regex_generator(length=5, letters=letters, nails_length=2) for _ in range(5000)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9u7f2F0SK0E"
      },
      "source": [
        "# generate samples from regexes and remove the repetitions\n",
        "X = [list(set([regex_decoder(regex) for _ in range(100)])) for regex in y]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg-KraGMSMte"
      },
      "source": [
        "# make the dataset as a Pandas Data Frame\n",
        "import pandas as pd\n",
        "trainset = pd.DataFrame({'X': X, 'y': y})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuYdTI34SSwl"
      },
      "source": [
        "# remove those who have only one sample as training and so there could be no generalization at all\n",
        "trainset = trainset[trainset.X.apply(lambda x: len(x) >= 2)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "ok6Crhh2SaO6",
        "outputId": "d4e542e5-7306-49e8-b08c-52d0e42965ee"
      },
      "source": [
        "# show histogram of number of samples\n",
        "trainset.X.apply(lambda x: len(x)).hist()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff1d319a2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghXXQvSXSdNA"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "ZbMxnzqcSmag",
        "outputId": "69aad0c0-c534-4f17-bcdd-e8d2b12ecbf2"
      },
      "source": [
        "trainset.describe()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4911</td>\n",
              "      <td>4911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>4911</td>\n",
              "      <td>4911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>[fygeig, fyreig]</td>\n",
              "      <td>ng[oz]xo[lh]a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       X              y\n",
              "count               4911           4911\n",
              "unique              4911           4911\n",
              "top     [fygeig, fyreig]  ng[oz]xo[lh]a\n",
              "freq                   1              1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LzTPL_SSm0y"
      },
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 25  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 4800  # Number of samples to train on."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4njuEebSqgJ",
        "outputId": "15838406-6302-41ea-ee90-382241dc65b5"
      },
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set(' ')\n",
        "target_characters = set(' ')\n",
        "\n",
        "for index, row in trainset.iterrows():\n",
        "    input_text, target_text = row[0], row[1]\n",
        "    # convert the input from list to string\n",
        "    input_text = '•'.join(input_text)\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0  # one-hot encoding\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0  # padding the rest with space\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 4911\n",
            "Number of unique input tokens: 28\n",
            "Number of unique output tokens: 31\n",
            "Max sequence length for inputs: 207\n",
            "Max sequence length for outputs: 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAstcxTnStbg"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmKxZpiHSxUc",
        "outputId": "06ed8869-df6a-4305-8109-70d399d16943"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.1,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"lstm_regex\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "70/70 [==============================] - 72s 974ms/step - loss: 1.9225 - accuracy: 0.5476 - val_loss: 1.6446 - val_accuracy: 0.6116\n",
            "Epoch 2/25\n",
            "70/70 [==============================] - 68s 969ms/step - loss: 1.5091 - accuracy: 0.6167 - val_loss: 1.4652 - val_accuracy: 0.6379\n",
            "Epoch 3/25\n",
            "70/70 [==============================] - 68s 973ms/step - loss: 1.3852 - accuracy: 0.6576 - val_loss: 1.3509 - val_accuracy: 0.6596\n",
            "Epoch 4/25\n",
            "70/70 [==============================] - 68s 972ms/step - loss: 1.2590 - accuracy: 0.6777 - val_loss: 1.2682 - val_accuracy: 0.6707\n",
            "Epoch 5/25\n",
            "70/70 [==============================] - 67s 963ms/step - loss: 1.2013 - accuracy: 0.6845 - val_loss: 1.2344 - val_accuracy: 0.6805\n",
            "Epoch 6/25\n",
            "70/70 [==============================] - 68s 965ms/step - loss: 1.1877 - accuracy: 0.6859 - val_loss: 1.1985 - val_accuracy: 0.6863\n",
            "Epoch 7/25\n",
            "70/70 [==============================] - 68s 968ms/step - loss: 1.1725 - accuracy: 0.6888 - val_loss: 1.1787 - val_accuracy: 0.6873\n",
            "Epoch 8/25\n",
            "70/70 [==============================] - 68s 972ms/step - loss: 1.1662 - accuracy: 0.6902 - val_loss: 1.1835 - val_accuracy: 0.6860\n",
            "Epoch 9/25\n",
            "70/70 [==============================] - 68s 970ms/step - loss: 1.1679 - accuracy: 0.6887 - val_loss: 1.1952 - val_accuracy: 0.6831\n",
            "Epoch 10/25\n",
            "70/70 [==============================] - 68s 973ms/step - loss: 1.1616 - accuracy: 0.6910 - val_loss: 1.1877 - val_accuracy: 0.6847\n",
            "Epoch 11/25\n",
            "70/70 [==============================] - 69s 981ms/step - loss: 1.1642 - accuracy: 0.6900 - val_loss: 1.1794 - val_accuracy: 0.6857\n",
            "Epoch 12/25\n",
            "70/70 [==============================] - 68s 977ms/step - loss: 1.1581 - accuracy: 0.6908 - val_loss: 1.1882 - val_accuracy: 0.6864\n",
            "Epoch 13/25\n",
            "70/70 [==============================] - 69s 989ms/step - loss: 1.1588 - accuracy: 0.6917 - val_loss: 1.2069 - val_accuracy: 0.6823\n",
            "Epoch 14/25\n",
            "70/70 [==============================] - 68s 975ms/step - loss: 1.1576 - accuracy: 0.6910 - val_loss: 1.1733 - val_accuracy: 0.6871\n",
            "Epoch 15/25\n",
            "70/70 [==============================] - 69s 981ms/step - loss: 1.1525 - accuracy: 0.6922 - val_loss: 1.1763 - val_accuracy: 0.6886\n",
            "Epoch 16/25\n",
            "70/70 [==============================] - 69s 983ms/step - loss: 1.1597 - accuracy: 0.6910 - val_loss: 1.1711 - val_accuracy: 0.6887\n",
            "Epoch 17/25\n",
            "70/70 [==============================] - 69s 990ms/step - loss: 1.1482 - accuracy: 0.6930 - val_loss: 1.1908 - val_accuracy: 0.6852\n",
            "Epoch 18/25\n",
            "70/70 [==============================] - 71s 1s/step - loss: 1.1581 - accuracy: 0.6912 - val_loss: 1.1995 - val_accuracy: 0.6852\n",
            "Epoch 19/25\n",
            "70/70 [==============================] - 69s 993ms/step - loss: 1.1522 - accuracy: 0.6918 - val_loss: 1.1850 - val_accuracy: 0.6871\n",
            "Epoch 20/25\n",
            "70/70 [==============================] - 69s 987ms/step - loss: 1.1506 - accuracy: 0.6925 - val_loss: 1.1765 - val_accuracy: 0.6884\n",
            "Epoch 21/25\n",
            "70/70 [==============================] - 70s 1s/step - loss: 1.1476 - accuracy: 0.6925 - val_loss: 1.1777 - val_accuracy: 0.6868\n",
            "Epoch 22/25\n",
            "70/70 [==============================] - 70s 996ms/step - loss: 1.1468 - accuracy: 0.6932 - val_loss: 1.1854 - val_accuracy: 0.6841\n",
            "Epoch 23/25\n",
            "70/70 [==============================] - 70s 999ms/step - loss: 1.1427 - accuracy: 0.6942 - val_loss: 1.1775 - val_accuracy: 0.6890\n",
            "Epoch 24/25\n",
            "70/70 [==============================] - 71s 1s/step - loss: 1.1435 - accuracy: 0.6942 - val_loss: 1.1866 - val_accuracy: 0.6871\n",
            "Epoch 25/25\n",
            "70/70 [==============================] - 70s 998ms/step - loss: 1.1463 - accuracy: 0.6944 - val_loss: 1.1845 - val_accuracy: 0.6869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: lstm_regex/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: lstm_regex/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pw8GCiJSz4e"
      },
      "source": [
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"lstm_regex\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itGGG7FkS5bq",
        "outputId": "a33f2fd1-8093-4837-e82b-79b1e0ea25eb"
      },
      "source": [
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: rdfnydr•rdfnbdr\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: ohhzzvlw•ohhzzplw•oyhzzvlw•oyhzzplw\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: wvavkx•wlavkx\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: tqkbsfn•tqzbsfn\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: ehcsyk•ehlsyk\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: yloth•ylota\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: napwpe•napope•nupope•nupwpe\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: lnbnoin•lrbnoin\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: kvgqkfl•kvgnrfl•kvgnkfl•kvgqrfl\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: imrpyok•imppyok•imrpjok•imppjok\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: zwdypolk•zwpyqolk•zwdyqolk•zwpypolk\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: wxzki•wxrki\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: eibyuy•eibyly•eabyuy•eabyly\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: uxwz•uxpz\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: kybifgg•kybifyg•kvbifgg•kvbifyg\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: nzjwslg•nzjxslg\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: fogjfsu•fojjfsu•fojjfju•fogjfju\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: squgdqcy•sqygdrcy•squgdrcy•sqygdqcy\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: innibu•innieu\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n",
            "-\n",
            "Input sentence: dloxlwa•dyoxlwa•dloxiwa•dyoxiwa\n",
            "Decoded sentence: t[mm][mm]nmm\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxDFecPUTRrr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
